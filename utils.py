# -*- coding: utf-8 -*-
"""final_lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_oacw32-kc6slScqr-mRSo5N6FJFIy2g
"""

import os
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split

import math
import pandas as pd
import matplotlib.pyplot as plt

from datasets import load_dataset
from tokenizers import Tokenizer
from tokenizers.models import WordLevel
from tokenizers.trainers import WordLevelTrainer
from tokenizers.pre_tokenizers import Whitespace

from pathlib import Path
from typing import Any
from tqdm import tqdm

import kagglehub
import re, string

class InputEmbedding(nn.Module):
  def __init__(self, d_model, vocab_size):
    """
    vocab_size: số lượng token trong từ điển (số lượng vocab trong tokenizer)
    d_model: số chiều của embedding
    mỗi token được biểu diễn dưới dạng vector có kích thước d_model
    """
    super().__init__()
    self.d_model = d_model
    self.vocab_size = vocab_size
    # tạo một ma trận ánh xạ có kích thước (vocab_size, d_model)
    # mỗi dòng trong ma trận biễu diễn một vector embedding của một token trong từ điển.
    self.embedding = nn.Embedding(vocab_size, d_model)

  def forward(self, x):
    """
    x: (batch_size, n)
    output: (batch_size, n, d_model)
    """
    return self.embedding(x) * math.sqrt(self.d_model)

class PositionalEncoding(nn.Module):
  def __init__(self, d_model, n, dropout):
    """
    n: số lượng token trong câu
    d_model: số chiều của embedding
    """
    super().__init__()
    self.d_model = d_model
    self.n = n
    self.dropout = nn.Dropout(dropout)

    # tạo position encoding tensor kích thước (n * d_model), khởi tạo là các số 0
    pe = torch.zeros(n, d_model)

    # tạo postion tensor kích thước (1 * n), khởi tạo là [[0.0, 1.0, ..., n - 1]]
    pos = torch.arange(0, n).float().unsqueeze(1)

    # tạo tensor là mẫu của Positional Encoding: 100000^(2i / d_model), với i thuộc {0, 1, ..., (n - 1) / 2}
    div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
    # tại mỗi dòng i của pe, các cột lẻ: pe(pos, 2i) = sin(pos(i) * div(2*i))
    pe[:, 0::2] = torch.sin(pos * div)
    # tại mỗi dòng i của pe, các cột chẵn: pe(pos, 2i + 1) = cos(pos(i) * div(2*i))
    pe[:, 1::2] = torch.cos(pos * div)
    # thêm một chiều cho tensor pe, pe là ma trận 3 chiều (batch_size, n, d_model)
    pe = pe.unsqueeze(0)
    # lưu pe vào bộ nhớ buffer, buffer là tensor không phải parameter của mô hình
    self.register_buffer("pe", pe)

  def forward(self, x):
    """
    x: (batch_size, n, d_model)
    output: (batch_size, n, d_model)
    """
    x = x + self.pe[:, :x.shape[1], :].requires_grad_(False)
    return self.dropout(x)

class LayerNormalization(nn.Module):
  def __init__(self, epsilon: float = 1e-9):
    super().__init__()
    self.epsilon = epsilon
    # alpha, beta là các trainable parameter, alpha khởi tạo là 1, beta là 0
    self.alpha = nn.Parameter(torch.ones(1))
    self.beta = nn.Parameter(torch.zeros(1))
  def forward(self, x):
    """
    x: (batch_size, n, d_model)
    output: (batch_size, n, d_model)
    """
    # Tính mean/std của mỗi token trong input data, dim = -1 tức là tính mean/std của các giá trị dọc theo d_model
    mean = x.mean(dim=-1, keepdim=True)
    std = x.std(dim=-1, keepdim=True)
    # Chuẩn hóa x
    x_scaled = (x - mean) / (std + self.epsilon)
    return self.alpha * x_scaled + self.beta

class FeedForwardSubLayer(nn.Module):
  def __init__(self, d_model, d_ff, dropout=0.1):
    """
    d_model: số chiều của embedding
    d_ff: số chiều của feed forward
    * dropout (kĩ thuật regularization giảm overfit):
      - xác suất ngẫu nhiên tắt một số neuron trong mỗi lần training, neuron bị tắt sẽ không được cập nhật trong lần đó
      - nếu p = dropout, thì 1 neuron có xác suất bị tắt là p
    * linear_layer(d_model, d_ff): lớp fully connected layer nhận input có kích thước d_model và output có kích thước d_ff với:
      y = xW^T + b
    Khi đó: - x có kích thước (batch_size, n, d_model)
            - y có kích thước (batch_size, n, d_ff)
            - W^T có kích thước (d_model, d_ff)
            - b có kích thước (d_ff)
    * relu(x): hàm activation, y = max(0, x)
    """
    super().__init__()

    self.linear1 = nn.Linear(d_model, d_ff)
    self.linear2 = nn.Linear(d_ff, d_model)
    self.dropout = nn.Dropout(dropout)

  def forward(self, x):
    """
    linear -> relu (dropout) -> linear
    (batch_size, n, d_model) -> (batch_size, n, d_ff) -> (batch_size, n, d_model)
    """
    x = self.linear1(x)
    x = torch.relu(x)
    x = self.dropout(x)
    x = self.linear2(x)
    return x

class MultiHeadAttentionSubLayer(nn.Module):
  def __init__(self, d_model, h, dropout=0.1):
    """
    d_model: số chiều của embedding
    h: số lượng head
    """
    super().__init__()
    self.d_model = d_model
    self.h = h
    if d_model % h != 0:
      raise ValueError("d_model phải chia hết cho h")
    # d_k là số chiều của mỗi key, query, value vectors trong head_i
    self.d_k = d_model // h

    # các ma trận trọng số kích thước (d_model * d_model)
    # lưu ý: W_Q = (W_Q_1, ..., W_Q_h) với W_Q_i là ma trận trọng số cho Q của head_i
    self.W_Q = nn.Linear(d_model, d_model)
    self.W_K = nn.Linear(d_model, d_model)
    self.W_V = nn.Linear(d_model, d_model)
    self.W_O = nn.Linear(d_model, d_model)

    self.dropout = nn.Dropout(dropout)

  def attention(self, Q, K, V, mask=None):
    """
    Q, K, V: (batch_size, n, d_k)
    mask: (batch_size, n, n)
    output: (batch_size, n, d_k)
    """
    d_k = Q.shape[-1]
    # Tính QK^T/sqrt(d_k) trước khi áp dụng softmax
    S = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    # S: (batch_size, n, n)
    # Masking được sử dụng cho Masked MultiHead Attention trong decoder để ngăn cho các từ ở vị trí hiện tại trong ma trận S nhìn thấy tương lai
    if mask is not None:
      
      # Adjust mask dimensions if needed
      if len(mask.shape) == 3 and len(S.shape) == 4:
          # mask: (batch, seq, seq) -> (batch, heads, seq, seq)
          mask = mask.unsqueeze(1).expand(-1, S.size(1), -1, -1)
      S = S.masked_fill(mask == 0, -1e9)
    # Tính softmax dọc theo các giá trị trên chiều cuối của ma trận. giả sử S: (a, b, c) thì sẽ tính soft max dọc theo chiều c, tức là tổng prob trên 1 dòng bằng 1
    attention_scores = S.softmax(dim=-1)
    attention_scores = self.dropout(attention_scores)
    # trả về attention = softmax(S)V
    return torch.matmul(attention_scores, V)

  def forward(self, Q, K, V, mask=None):
    """
    Q, K, V: tensor kích thước (batch_size, n, d_model)
    """
    # Tính Q', K', V', với Q' = W_Q @ Q = (W_Q_1 @ Q, ..., W_Q_h @ Q)
    _Q = self.W_Q(Q)
    _K = self.W_K(K)
    _V = self.W_V(V)
    # chia Q', K', V' làm h heads, mỗi head có số chiều là d_k (d_model = d_k * h)
    # Q' -> Q_split: (batch_size, n, d_model) -> (batch_size, n, h, d_k)
    # transpose(1, 2) chuyển vị n <-> h:  (batch_size, n, h, d_k) <-> (batch_size, h, n, d_k) để đưa head lên chiều thứ 2
    Q_split = _Q.view(_Q.shape[0], _Q.shape[1], self.h, self.d_k).transpose(1, 2)
    K_split = _K.view(_K.shape[0], _K.shape[1], self.h, self.d_k).transpose(1, 2)
    V_split = _V.view(_V.shape[0], _V.shape[1], self.h, self.d_k).transpose(1, 2)
    # như vậy khi tính attention, attention được tính lần lượt theo mỗi (batch size, số head)
    attention = self.attention(Q_split, K_split, V_split, mask)
    # trả về ma trận số chiều gốc (batch_size, h, n, d_k) -> (batch_size, n, h, d_k) -> (batch_size, n, d_model)
    attention = attention.transpose(1, 2).contiguous().view(attention.shape[0], -1, self.d_model)
    # trả vè multi-head attention
    return self.W_O(attention)

class ResidualConnection(nn.Module):
  def __init__(self, dropout):
    super().__init__()
    self.dropout = nn.Dropout(dropout)
    self.LayerNorm = LayerNormalization()

  def forward(self, x, Sublayer):
    """
    x: (batch_size, n, d_model)
    sublayer: lớp sublayer của Encoder/Decoder Layer
    output: (batch_size, n, d_model)
    """
    # output of sublayer is: LayerNorm(x + Sublayer(x))
    r = self.dropout(Sublayer(x))
    return self.LayerNorm(x + r)

class EncoderLayer(nn.Module):
  def __init__(self,
               self_attention_sublayer: MultiHeadAttentionSubLayer,
               feed_forward_sublayer: FeedForwardSubLayer,
               dropout: float
               ):
    super().__init__()
    self.ResidualConnection1 = ResidualConnection(dropout)
    self.ResidualConnection2 = ResidualConnection(dropout)
    self.SelfAttentionSublayer = self_attention_sublayer
    self.FeedForwardSublayer = feed_forward_sublayer

  def forward(self, x, mask):
    """
    x: tensor kích thước (batch_size, n, d_model)
    mask: tensor kích thước (batch_size, n, n)
    encoder gồm 2 lớp sublayer có residual connection (Add&Norm):
      - multi-head attention
      - feed forward
    output: (batch_size, n, d_model)
    """
    x = self.ResidualConnection1(x, lambda x: self.SelfAttentionSublayer(x, x, x, mask))
    x = self.ResidualConnection2(x, self.FeedForwardSublayer)
    return x

class Encoder(nn.Module):
  def __init__(self, Layers: nn.ModuleList):
    """
    một lớp Encoder gồm N (thường là 6) EncoderLayer
    """
    super().__init__()
    self.Layers = Layers
    self.LayerNorm = LayerNormalization()

  def forward(self, x, mask):
    """
    x: tensor kích thước (batch_size, n, d_model)
    mask: tensor kích thước (batch_size, n, n)
    trong encoder, các layer lấy input từ layer phía trước
    output: (batch_size, n, d_model)
    """
    for layer in self.Layers:
      x = layer(x, mask)
    return self.LayerNorm(x)

class DecoderLayer(nn.Module):
  def __init__(self,
               self_attention_sublayer: MultiHeadAttentionSubLayer,
               cross_attention_sublayer: MultiHeadAttentionSubLayer,
               feed_forward_sublayer: FeedForwardSubLayer,
               dropout: float
               ):
    super().__init__()
    self.ResidualConnection1 = ResidualConnection(dropout)
    self.ResidualConnection2 = ResidualConnection(dropout)
    self.ResidualConnection3 = ResidualConnection(dropout)
    self.SelfAttentionSublayer = self_attention_sublayer
    self.CrossAttentionSublayer = cross_attention_sublayer
    self.FeedForwardSublayer = feed_forward_sublayer

  def forward(self, x, EncoderOutput, input_mask, output_mask):
    """
    x: tensor kích thước (batch_size, n, d_model)
    mask: tensor kích thước (batch_size, n, n)
    input_mask: masking của source
    output_mask: masking của target

    encoder gồm 3 lớp sublayer có residual connection (Add&Norm):
      - mask multi-head attention (sử dụng mask_output : ko nhìn các từ tương lai và ko nhìn padding)
      - cross attention (sử dụng encoder output) (sử dụng input_mask : ko nhìn các từ padding)
      - feed forward
    output: (batch_size, n, d_model)
    """
    x = self.ResidualConnection1(x, lambda x: self.SelfAttentionSublayer(x, x, x, output_mask))
    x = self.ResidualConnection2(x, lambda x: self.CrossAttentionSublayer(x, EncoderOutput, EncoderOutput, input_mask))
    x = self.ResidualConnection3(x, self.FeedForwardSublayer)
    return x

class Decoder(nn.Module):
  def __init__(self, Layers: nn.ModuleList):
    super().__init__()
    self.Layers = Layers
    self.LayerNorm = LayerNormalization()

  def forward(self, x, encoder_output, input_mask, output_mask):
    """
    x: (batch_size, n, d_model) -> query
    encoder_output: (batch_size, n, d_model) -> key, value
    output_mask: (batch_size, n, n)
    trong decoder, các layer lấy query từ layer phía trước, key và value là của encoder output
    output: (batch_size, n, d_model)
    """
    for layer in self.Layers:
      x = layer(x, encoder_output, input_mask, output_mask)
    return self.LayerNorm(x)

class ProjectionLayer(nn.Module):
  def __init__(self, d_model, vocab_size):
    """
    d_model: số chiều của embedding
    vocab_size: số lượng token trong từ điển (số lượng vocab trong tokenizer)
    """
    super().__init__()
    self.linear = nn.Linear(d_model, vocab_size)
  def forward(self, x):
    """
    x: (batch_size, n, d_model)
    output: (batch_size, n, vocab_size)

    hoặc khi sử dụng cho greedy decode:
    x: (batch_size, d_model)
    output: (batch_size, vocab_size)

    sử dụng lớp fully connected layer (linear layer) chuyển đổi mỗi vector embedding (chiều d_model) thành một vector logits (chiều vocab_size): (batch_size, n, d_model) -> (batch_size, n, vocab_size)
    logits = self.linear(x)
    """
    return self.linear(x)

class Transformer(nn.Module):
  def __init__(self,
               encoder: Encoder,
               decoder: Decoder,
               input_embed: InputEmbedding,
               output_embed: InputEmbedding,
               input_pe: PositionalEncoding,
               output_pe: PositionalEncoding,
               projection_layer: ProjectionLayer
               ):
    super().__init__()
    self.Encoder = encoder
    self.Decoder = decoder
    self.input_embed = input_embed
    self.input_pe = input_pe
    self.output_embed = output_embed
    self.output_pe = output_pe
    self.ProjectionLayer = projection_layer

  def encode(self, x, mask):
    """
    x: tensor kích thước (batch_size, n)
    mask: tensor kích thước (batch_size, n, n)
    """
    # tạo embedding cho x
    x = self.input_embed(x)
    # thêm position encoding cho x
    x = self.input_pe(x)
    # encode với mask
    return self.Encoder(x, mask)

  def decode(self, encoder_output, input_mask, x, output_mask):
    """
    x: tensor kích thước (batch_size, n)
    encoder_output: tensor kích thước (batch_size, n, d_model)
    input_mask: tensor kích thước (batch_size, n, n)
    output_mask: tensor kích thước (batch_size, n, n)
    """
    # tạo embedding cho x
    x = self.output_embed(x)
    # thêm position encoding cho x
    x = self.output_pe(x)
    # decode với mask
    return self.Decoder(x, encoder_output, input_mask, output_mask)

  def project(self, x):
    return self.ProjectionLayer(x)

def setup_transformer(input_vocab_size, output_vocab_size, input_n, output_n, d_model, N, h, d_ff, dropout) -> Transformer:
  """
  khởi tạo transformer
  input_vocab_size: số lượng token trong từ điển của input tokenizer
  output_vocab_size: số lượng token trong từ điển của output tokenizer
  input_n: số lượng token trong câu input
  output_n: số lượng token trong câu output
  d_model: số chiều của embedding
  N: số lượng encoder/decoder layer
  h: số lượng head
  d_ff: số chiều feed forward của inner layer
  """

  # tạo input_embedding, output_embedding và positional encoding tương ứng
  input_embed = InputEmbedding(d_model, input_vocab_size)
  output_embed = InputEmbedding(d_model, output_vocab_size)
  input_pe = PositionalEncoding(d_model, input_n, dropout)
  output_pe = PositionalEncoding(d_model, output_n, dropout)

  # tạo các lớp encoder layers cho Encoder
  encoder_layers = nn.ModuleList([
    EncoderLayer(
      MultiHeadAttentionSubLayer(d_model, h, dropout),
      FeedForwardSubLayer(d_model, d_ff, dropout),
      dropout
    ) for _ in range(N)])

  # tạo các lớp decoder layers cho Decoder
  decoder_layers = nn.ModuleList([
    DecoderLayer(
      MultiHeadAttentionSubLayer(d_model, h, dropout),
      MultiHeadAttentionSubLayer(d_model, h, dropout),
      FeedForwardSubLayer(d_model, d_ff, dropout),
      dropout
    ) for _ in range(N)])

  # set up Transformer
  encoder = Encoder(encoder_layers)
  decoder = Decoder(decoder_layers)
  projection_layer = ProjectionLayer(d_model, output_vocab_size)

  transformer = Transformer(encoder, decoder, input_embed, output_embed, input_pe, output_pe, projection_layer)

  # khởi tạo các parameter cho mô hình
  for p in transformer.parameters():
    if p.dim() > 1:
      nn.init.xavier_uniform_(p)
  return transformer

"""
<UNK>: unknow token

<PAD>: padding token, đảm bảo tất cả các câu trong 1 batch có cùng độ dài n

<SOS>: Start of Sentence

<EOS>: End of Sentence
"""


def causal_mask(size):
    # mask tam giác trên (j > i) = False
    m = torch.triu(torch.ones(size, size), diagonal=1).bool()
    return ~m
  

def preprocessing(df):
    """
    preprocessing tập dataset 
    """
    
    df["en"] = df["en"].astype(str)
    df["vi"] = df["vi"].astype(str)
    
    # loại bỏ punctuation
    df["en"] = df["en"].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))
    df["vi"] = df["vi"].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))
    
    # chuyển về lowercase
    df["en"] = df["en"].apply(lambda x: x.lower())
    df["vi"] = df["vi"].apply(lambda x: x.lower())
    
    # loại bỏ khoảng trắng thừa
    df["en"] = df["en"].apply(lambda x: x.strip())
    df["vi"] = df["vi"].apply(lambda x: x.strip())
    
    df["en"] = df["en"].apply(lambda x: re.sub(r"\s+", " ", x))
    df["vi"] = df["vi"].apply(lambda x: re.sub(r"\s+", " ", x))
    
    # Loại bỏ nếu một trong hai mẫu bị NaN
    df = df[(df["en"].str.len() > 0) & (df["vi"].str.len() > 0)]
    
    print("Sample data after preprocessing:")
    print(df)
    
    return df.reset_index(drop=True)


def setup_example(example, n, tok_in, tok_out, in_col="en", out_col="vi"):
  """
  tạo một mẫu cho dataset
  {
      "encoder_input": input text được input thành tokens có '<sos> encode_input_text <eos>',
      "decoder_input": output text được input thành tokens có '<sos> encode_output_text',
      "label": output text được input thành tokens có 'encode_output_text <eos>',
      "encoder_mask": masking ko nhìn thấy các padding,
      "decoder_mask": masking ko nhìn thấy từ tương lai và padding,
      "text_input": input_text,
      "text_output": output_text
  }
  """
  
  # token đặc biệt
  sos = tok_out.token_to_id("<SOS>")
  eos = tok_out.token_to_id("<EOS>")
  pad = tok_out.token_to_id("<PAD>")
  
  # Debug: kiểm tra các token đặc biệt có trong câu hay không
  if any(token_id is None for token_id in [sos, eos, pad]):
      print(f"ERROR: Special tokens not found!")
      print(f"SOS: {sos}, EOS: {eos}, PAD: {pad}")
      print(f"Available tokens: {list(tok_out.get_vocab().keys())[:20]}")  
      raise ValueError("Special tokens not found in tokenizer!")
  
  # input_text, output_text tương ứng câu tiếng anh và câu tiếng việt
  input_text = example[in_col]
  output_text = example[out_col]
  
  # encode text thành ids (ids là dạng biễu diễn số của token)
  try:
      inp_ids = tok_in.encode(str(input_text)).ids
      out_ids = tok_out.encode(str(output_text)).ids
  except Exception as e:
      print(f"ERROR encoding text: {e}")
      print(f"Input text: '{input_text}'")
      print(f"Output text: '{output_text}'")
      raise e
  
  # truncate text nếu vượt quá độ dài qui định
  inp_ids = inp_ids[: (n - 2)]  # chừa chỗ cho SOS, EOS
  dec_in_ids = out_ids[: (n - 1)]  # chừa chỗ cho SOS
  lbl_ids = out_ids[: (n - 1)]  # chừa chỗ cho EOS

  enc = [sos] + inp_ids + [eos]
  dec_in = [sos] + dec_in_ids
  lbl = lbl_ids + [eos]

  # Debug: tìm enc or dec or lbl là None
  if None in enc or None in dec_in or None in lbl:
      print(f"ERROR: None values found")
      print(f"enc: {enc}")
      print(f"dec_in: {dec_in}")
      print(f"lbl: {lbl}")
      raise ValueError("None values found")

  # thêm padding để đủ độ dài câu
  enc += [pad] * (n - len(enc))
  dec_in += [pad] * (n - len(dec_in))
  lbl += [pad] * (n - len(lbl))

  # Debug: kiểm tra bug tensor
  try:
      encoder_input = torch.tensor(enc, dtype=torch.long)
      decoder_input = torch.tensor(dec_in, dtype=torch.long)
      label = torch.tensor(lbl, dtype=torch.long)
  except Exception as e:
      print(f"ERROR creating tensors: {e}")
      print(f"enc: {enc}")
      print(f"dec_in: {dec_in}")
      print(f"lbl: {lbl}")
      raise e

  # encoder mask, nếu không phải padding thì = 1 (do không muốn nhìn các từ padding)
  enc_mask = (encoder_input != pad).unsqueeze(0).unsqueeze(0)  # [1, 1, n]
  # decoder padding masking, nếu không phải padding thì = 1
  dec_pad = (decoder_input != pad).unsqueeze(0)               # [1, n]
  # ma trận tam giác dưới bằng 1
  dec_caus = causal_mask(n).unsqueeze(0)                      # [1, n, n]
  # ma trận tam giác dưới loại bỏ các từ padding (không muốn nhìn thấy tương lai và không muốn nhìn các từ padding)
  dec_mask = (dec_pad.unsqueeze(-1) & dec_caus)              # [1, n, n]

  assert encoder_input.size(0) == n
  assert decoder_input.size(0) == n
  assert label.size(0) == n

  return {
      "encoder_input": encoder_input, # [1, n]
      "decoder_input": decoder_input, # [1, n]
      "label": label, # [1, n]
      "encoder_mask": enc_mask,
      "decoder_mask": dec_mask,
      "text_input": input_text,
      "text_output": output_text
  }

def build_dataset(df, n, tok_in, tok_out, input_col="en", output_col="vi", min_freq=2):
  samples = []
  failed_samples = 0
  
  print(f"Building dataset from {len(df)} samples...")
  
  for idx, row in tqdm(df.iterrows()):
      try:
          sample = setup_example(row, n, tok_in, tok_out, in_col=input_col, out_col=output_col)
          samples.append(sample)
              
      except Exception as e:
          # nếu build ko đc mẫu đó thì fail+=1
          failed_samples += 1
          print(f"Failed to process sample {idx}: {e}")
          print(f"Sample data: {row[input_col]} -> {row[output_col]}")
          
          if failed_samples > 10:
              print("Too many failed samples, stopping...")
              break
          continue

  print(f"Successfully processed {len(samples)} samples, {failed_samples} failed")
  return samples

class TranslationDataset(Dataset):
    """
    Để sử dụng Dataloader thì dataset là 1 class Custom Dataset
    """
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]


def setup_tokenizer(texts=None, min_frequency=2, name=''):
    """
    Load tokenizer từ file name, nếu không tìm thấy thì train tokenizer from texts
    """
    try:
      tokenizer = Tokenizer.from_file(name)
    except:
      tokenizer = Tokenizer(WordLevel(unk_token="<UNK>"))
      tokenizer.pre_tokenizer = Whitespace()
      trainer = WordLevelTrainer(
          special_tokens=["<UNK>", "<PAD>", "<SOS>", "<EOS>"],
          min_frequency=min_frequency
      )
      tokenizer.train_from_iterator(texts, trainer=trainer)
      tokenizer.save(name)
    return tokenizer

def greedy_decode(model, input, input_mask, tokenizer_input, tokenizer_output, n, device):
    """
    input: (batch_size, n)
    input_mask:  (batch_size, n, n)
    tokenizer_input/output: tokenizer đã train
    n: độ dài cố định của 1 sentence được generate
    """
    sos = tokenizer_output.token_to_id("<SOS>")
    eos = tokenizer_output.token_to_id("<EOS>")

    encoder_output = model.encode(input, input_mask)
    decoder_input = torch.empty(1,1).fill_(sos).type_as(input).to(device) # decode trước <SOS>, từ đó mô hình dự đoán từ tiếp theo dựa trên <SOS>
    while True:
      # Loop đến khi đạt độ dài tối đa n
      if decoder_input.size(1) == n:
            break
      # tạo mask cho decoder input
      output_mask = causal_mask(decoder_input.size(1)).type_as(input_mask).to(device)
      output = model.decode(encoder_output, input_mask, decoder_input, output_mask)
      # predict xác suất (lấy xác suất cao nhất) cho token tiếp theo dựa vào token trước đó
      # output[:, -1] có kích thước (batch_size, d_model) : token cuối cùng trong các câu (có batch_size câu)
      logits = model.project(output[:, -1])
      probs = torch.softmax(logits, dim=-1) # (batch_size, vocab_size)
      # logit: (batch_size, vocab_size)
      _, next_token = torch.max(probs, dim=1) # lấy max theo dọc theo chiều batch_size (note: số câu tương đương batch_size, từ điển có vocab_size từ), mỗi câu lấy trong từ điển từ có xác suất cao nhất
      next_token = next_token.item()
      decoder_input = torch.cat(
          [decoder_input, torch.empty(1,1).type_as(input).fill_(next_token).to(device)], dim=1
      )
      # kiểm tra nếu next token là <EOS> thì dừng
      if next_token == eos:
          break
    return decoder_input.squeeze(0)

import nltk
from nltk.translate.bleu_score import corpus_bleu


def predict(model, n, tokenizer_input, tokenizer_output, sentence, device, max_len=50): 
    """
    Dự đoán câu dịch cho câu đầu vào bằng mô hình Transformer
    """
    model.eval()

    # Tokenize câu đầu vào (tiếng Anh)
    example = {
      "en": sentence, "vi": ""
    }
    input = setup_example(example, n, tokenizer_input, tokenizer_output, in_col="en", out_col="vi")

    # Generate output (dùng greedy decoding)
    output = greedy_decode(model, input["encoder_input"].to(device), input["encoder_mask"].to(device), tokenizer_input, tokenizer_output, max_len, device)

    # Decode output thành văn bản
    output_text = tokenizer_output.decode(output.cpu().tolist())

    return output_text

def eval(model, data, tokenizer_input, tokenizer_output, n, device):
    """
    model: model muốn eval
    data: data muốn eval
    tokenizer_input: tokenizer của model muốn eval
    tokenizer_output: tokenizer của model muốn eval
    n: số lượng token tối đa trong 1 câu
    """
    model.eval()
    
    val_loader = DataLoader(data, batch_size=1, shuffle=False)
    
    references = []
    hypotheses = []
    with torch.no_grad():
        for batch in val_loader:
            encoder_input = batch["encoder_input"].to(device)
            encoder_mask = batch["encoder_mask"].squeeze(1).to(device)
            decoder_input = batch["decoder_input"].to(device)
            decoder_mask = batch["decoder_mask"].squeeze(1).to(device)
            # label = batch["label"].to(device)

            encoder_output = model.encode(encoder_input, encoder_mask)
            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)
            # output = model.project(decoder_output)
            
            # Tính BLEU score
            text_inputs = batch["text_input"]
            text_outputs = batch["text_output"]

            for i in range(encoder_input.size(0)):
                input_tensor = encoder_input[i].unsqueeze(0)
                input_mask_tensor = encoder_mask[i].unsqueeze(0)
                # Generate output (dùng greedy decoding)
                output_ids = greedy_decode(model, input_tensor, input_mask_tensor,
                                            tokenizer_input, tokenizer_output, n, device)
                # Decode output thành văn bản
                output_text = tokenizer_output.decode(output_ids.cpu().tolist())
                
                pred_tokens = tokenizer_output.encode(output_text).tokens
                true_tokens = tokenizer_output.encode(text_outputs[i]).tokens
                hypotheses.append(pred_tokens)
                references.append([true_tokens])
                
                print(f"Input: {text_inputs[i]}")
                print(f"Output: {text_outputs[i]}")
                print(f"Predicted: {output_text}")

        bleu = corpus_bleu(references, hypotheses)
        print(f"BLEU Score: {bleu:.4f}")

            
from torch.optim.lr_scheduler import ReduceLROnPlateau

def train(model, train_data, val_data, tokenizer_input, tokenizer_output, optimizer, criterion, n, batch_size, epochs, device, save_dir="models"):
    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)
    
    model.train()
    train_losses = []
    val_losses = []
    # hàm loss dynamic
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)

    for epoch in range(epochs):
        model.train()
        
        # train 1 epoch
        epoch_loss = 0.0
        for batch in tqdm(train_loader):
            encoder_input = batch["encoder_input"].to(device)  # [batch_size, n]
            encoder_mask = batch["encoder_mask"].to(device)    # [batch_size, 1, 1, n]
            decoder_input = batch["decoder_input"].to(device)  # [batch_size, n]
            decoder_mask = batch["decoder_mask"].to(device)    # [batch_size, 1, n, n]
            # label là 'encoded_output_text <eos>'
            label = batch["label"].to(device)                  # [batch_size, n]

            # đưa mask về dạng 3 chiều
            encoder_mask = encoder_mask.squeeze(1)  # [batch_size, 1, n]
            decoder_mask = decoder_mask.squeeze(1)  # [batch_size, n, n]

            # encode và decode
            encoder_output = model.encode(encoder_input, encoder_mask)
            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)
            
            # Tính logits (lưu ý logits chưa phải là xác suất mà là output của lớp linear cuối cùng)
            output = model.project(decoder_output) # [batch_size, n, vocab_size]
            # decoder output có kích thước [batch_size, n, d_model], sau khi đi qua proj_layer
            # nó trở thành logits (raw score) với kích thước [batch_size, n, vocab_size]
            # logits là score chưa được chuẩn hóa cho mỗi token trong vocabulary tại mỗi vị trí i trong câu (câu có n vị trí)
            
            # tính loss 
            vocab_size = tokenizer_output.get_vocab_size()
            output = output.view(-1, vocab_size) # flatten logit thành kích thước [batch_size * n, vocab_size]
            label = label.view(-1) # flatten kích thước thành [batch_size * n]

            # tính loss dựa vào logits (trong cross entropy sẽ apply softmax) và so sánh probabilities với true label
            loss = criterion(output, label)
            
            # cộng dồn loss của từng batch
            epoch_loss += loss.item()
            
            # backprobagation
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        avg_epoch_loss = epoch_loss / len(train_loader)
        train_losses.append(avg_epoch_loss)
        print(f"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_epoch_loss:.4f}")

        # eval sau 1 epoch
        model.eval()
        val_loss = 0.0
        references = [] # reference (tokens) lưu groundtruth của các câu trong tập eval 
        hypotheses = [] # hypothesis (tokens) lưu predict tiếng việt của các câu tiếng anh trong tập eval
        with torch.no_grad():
            for batch in val_loader:
                encoder_input = batch["encoder_input"].to(device)
                encoder_mask = batch["encoder_mask"].squeeze(1).to(device)
                decoder_input = batch["decoder_input"].to(device)
                decoder_mask = batch["decoder_mask"].squeeze(1).to(device)
                label = batch["label"].to(device)

                encoder_output = model.encode(encoder_input, encoder_mask)
                decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)
                output = model.project(decoder_output)

                output = output.view(-1, vocab_size)
                label = label.view(-1)
                loss = criterion(output, label)
                val_loss += loss.item()
                
                # tính BLEU score 
                text_outputs = batch["text_output"]

                for i in range(encoder_input.size(0)):
                    input_tensor = encoder_input[i].unsqueeze(0)
                    input_mask_tensor = encoder_mask[i].unsqueeze(0)
                    output_ids = greedy_decode(model, input_tensor, input_mask_tensor,
                                               tokenizer_input, tokenizer_output, n, device)
                    pred_tokens = tokenizer_output.encode(tokenizer_output.decode(output_ids.cpu().tolist())).tokens
                    true_tokens = tokenizer_output.encode(text_outputs[i]).tokens
                    hypotheses.append(pred_tokens)
                    references.append([true_tokens])

        bleu = corpus_bleu(references, hypotheses)
        print(f"BLEU Score: {bleu:.4f}")

        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        print(f"Validation Loss: {avg_val_loss:.4f}")

        scheduler.step(avg_val_loss)

        # save model
        if (epoch + 1) % 5 == 0 or (epoch + 1 == epochs):
            os.makedirs(save_dir, exist_ok=True)
            torch.save(model.state_dict(), os.path.join(save_dir, f"model_epoch_{epoch + 1}.pt"))
        
        # theo dõi prediction của câu muốn dịch
        sentence = "he is a menace"
        predicted_translation = predict(model, n, tokenizer_input, tokenizer_output, sentence, device)
        print(f"Predicted translation for '{sentence}': {predicted_translation}")

    # save loss plot
    plt.figure(figsize=(10, 5))
    plt.plot(range(epochs), train_losses, label="Train Loss", color="blue")
    plt.plot(range(epochs), val_losses, label="Validation Loss", color="red", linestyle="--")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Loss over Epochs")
    plt.legend()
    plt.grid(True)

    os.makedirs(save_dir, exist_ok=True)
    plt.savefig(os.path.join(save_dir, "loss_plot.png"))
    print("Loss plot saved to loss_plot.png")

    plt.show()


